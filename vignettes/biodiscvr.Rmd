---
title: "Biodiscvr Workflow with User Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Biodiscvr Workflow with User Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE # <<< IMPORTANT: Default to NOT evaluating chunks
)
# Load necessary packages for code display and explanation
library(biodiscvr)
library(yaml) # Needed for reading config examples potentially
```

## Introduction

This vignette demonstrates the typical end-to-end workflow for using the `biodiscvr` package to discover and evaluate biomarkers using **your own data**.

We will cover the following steps:

1.  Setting up necessary file paths.
2.  Loading datasets using `load_datasets()`.
3.  Preprocessing data (including checks, preparation, and filtering) using `preprocess_data()`.
4.  Generating demographic summaries using `create_demographics_table()`.
5.  Running discovery experiments using `run_experiments()`.
6.  Evaluating predefined literature biomarkers using `evaluate_literature_biomarkers()`.
7.  Identifying key output files for reporting.

**Note:** Because this vignette uses placeholders for user-specific file paths, the R code chunks are generally set to `eval=FALSE`. You will need to copy, paste, and adapt the code chunks into your own R script or console, replacing the placeholder paths with the actual locations of your files.

#### **Adapting the Framework to Other Domains**
While this vignette uses Alzheimer's Disease (AD) examples (e.g., amyloid status and neuroimaging regions), `biodiscvr` is designed to be a general-purpose biomarker discovery engine. To adapt this to a different field (such as Oncology or astronomy):

1.  **Define your Features:** Instead of brain regions, provide your domain-specific numeric features (e.g., gene expression levels, spectral intensities, imaging-derived metrics).
2.  **Define your Outcome:** In `config.yaml`, update the `group` definition to match your target outcome (e.g., positives vs. negatives, responders vs. non responders, high risk vs. low risk).
3.  **Map your Data:** Use `dict_suv.csv` (or an equivalent file) to map raw feature names to a standardised naming convention. This is especially useful when working with multiple datasets that use different labels: the framework uses this dictionary to harmonise names internally without modifying your original files. 

The underlying optimisation logic—finding optimal ratios that are robust across multiple independent datasets—remains the same regardless of the context.
If your application does not include longitudinal data, you may want to adapt the fitness function (e.g., using AUC, F1 score, or MAE instead of longitudinal consistency metrics).
For technical instructions on how to modify the source code or fitness metrics for non-AD research, please refer to the "Adapting to Other Research Domains" section in the [README](https://github.com/isaac-6/biodiscvr/tree/paper?tab=readme-ov-file#adapting-to-other-research-domains)


## Setup: User Files and Paths

Before running the workflow, ensure you have the following files prepared and know their locations:

1.  **Root Data Directory:** The main folder containing subdirectories for each dataset (e.g., "ADNI", "OASIS"). Each subdirectory should contain at least `data.csv` and `data_suv_bi.csv`.
2.  **Configuration File (`config.yaml`):** Contains parameters for preprocessing (including `date_column` for time calculation, inclusion criteria), model equations, power calculations, and GA settings.
3.  **SUV Dictionary (`dict_suv.csv`):** Defines SUV region names for matching/renaming.
4.  **Experiments File (`experiments.yaml`):** Defines the sequence of discovery runs.
5.  **Literature Biomarkers File (`literature_biomarkers.yaml`):** Defines standard biomarkers for comparison.

You will also need to decide on output locations:

6.  **Output Directory:** A folder where results (CSV files, plots) will be saved.
7.  **Log Directory (Optional):** A folder to store detailed log files from `preprocess_data`.

**Package-Provided Files:**

This workflow can utilise default configuration files included with the `biodiscvr` package (located in its `inst/files` directory):
*   `config.yaml`: Default configuration.
*   `dict_suv.csv`: Default SUV dictionary.
*   `experiments.yaml`: Default set of discovery experiments.
*   `literature_biomarkers.yaml`: (To be created) Default set of literature biomarkers.

You can inspect these default files using `system.file("files", "<filename>", package = "biodiscvr")`. If you need to customise them, copy them from the package installation directory to your own project space and provide the path to your custom file instead of using the package default in the function calls below.

**Define Your Paths (Replace Placeholders):**

```r
# --- USER: Define paths to YOUR files and directories ---

# 1. Path to the main folder containing dataset subfolders
user_data_root_dir <- "path/to/your/data_root_folder" # <<< REPLACE

# 2. Path to the directory containing your CUSTOM config.yaml and dict_suv.csv
#    IF YOU ARE NOT USING THE PACKAGE DEFAULTS.
#    If using package defaults for config/dict, this can be NULL or point to where
#    check_and_prepare_data (now part of preprocess_data) expects them.
#    For preprocess_data, it will use system.file if files_path is not given for config/dict.
user_supporting_files_dir <- "path/to/your/supporting_files" # <<< REPLACE IF USING CUSTOM CONFIG/DICT
                                                          # OR set to NULL to use package defaults

# 3. Path to your CUSTOM experiments definition file (if not using package default)
user_experiments_file <- "path/to/your/experiments.yaml" # <<< REPLACE IF CUSTOM
#    To use package default: system.file("files", "experiments.yaml", package = "biodiscvr")

# 4. Path to your CUSTOM literature biomarker definition file (if not using package default)
user_literature_defs_file <- "path/to/your/literature_biomarkers.yaml" # <<< REPLACE IF CUSTOM
#    To use package default: system.file("files", "literature_biomarkers.yaml", package = "biodiscvr")


# 5. Path to the desired main output directory (will be created if needed)
user_output_dir <- "path/to/your/output_folder" # <<< REPLACE
if (!dir.exists(user_output_dir)) dir.create(user_output_dir, recursive = TRUE)

# 6. Path to the desired directory for detailed logs (optional, set to NULL to disable)
user_log_dir <- file.path(user_output_dir, "logs")

# 7. Define desired output CSV filenames
main_results_csv <- "discovery_evaluation_results.csv"
literature_results_csv <- file.path(user_output_dir, "literature_evaluation_results.csv")
demographics_dir <- file.path(user_output_dir, "demographics")

# 8. Define datasets to process (must match folder names in user_data_root_dir)
datasets_to_process <- c("ADNI", "MAYO") # <<< REPLACE

# 9. Define an overall tag for this experimental run
run_tag <- format(Sys.time(), "%Y%m%d_%H%M")

# --- Get Paths to Package-Internal Files (if user_supporting_files_dir is NULL for config/dict) ---
# preprocess_data will handle finding these if files_path is not provided to it.
# We still need the path for experiments_file if using package default.
pkg_experiments_file <- system.file("files", "experiments.yaml", package = "biodiscvr", mustWork = TRUE)
pkg_literature_defs_file <- system.file("files", "literature_biomarkers.yaml", package = "biodiscvr", mustWork = TRUE) # Assuming it exists

# --- Load Config Once (needed by multiple steps) ---
# If using a custom config file:
# custom_config_path <- file.path(user_supporting_files_dir, "config.yaml")
# if (!file.exists(custom_config_path)) stop("Custom config file not found!")
# config <- yaml::read_yaml(custom_config_path)
# If using package default config (preprocess_data will load it):
# For other functions, we'll get it from preprocess_data output.
# For now, let's assume preprocess_data handles config loading.

```

## Step 1: Loading Datasets

This step scans your `user_data_root_dir` for subdirectories matching `datasets_to_process` and loads the `data.csv` and `data_suv_bi.csv` files from each.

```{r load_data, eval=FALSE}
# Load the datasets specified
loaded_data_raw <- load_datasets(root_path = user_data_root_dir)
# print(str(loaded_data_raw, max.level = 2))
```

*Expected Output:* A list named `loaded_data_raw`.

## Step 2: Preprocessing Data (Checks, Preparation, Filtering)

This single function now handles:
*   Validating loaded data against configuration and dictionary.
*   Calculating the `time` variable from a date column (e.g., `ScanDate`).
*   Performing SUV column matching and renaming/subsetting.
*   Applying filtering based on minimum entries and group-specific inclusion criteria.
*   Sets up logging.

If `user_supporting_files_dir` is provided and contains `config.yaml` and `dict_suv.csv`, those will be used. Otherwise, `preprocess_data` will attempt to use the default versions from `inst/files/` within the package.

```{r preprocess_data_merged, eval=FALSE}
# Run the consolidated preprocessing step
# If user_supporting_files_dir is NULL or doesn't contain config/dict,
# preprocess_data should default to package internal files.
# The 'files_path' argument in preprocess_data points to where config.yaml and dict_suv.csv are.
# If using package defaults, this path is derived internally.
# For this vignette, let's assume the user might provide their own, or the function handles defaults.

# Option 1: User provides path to their config/dict
# preprocessed_output <- preprocess_data(
#   loaded_data = loaded_data_raw,
#   files_path = user_supporting_files_dir, # Path to dir with user's config.yaml & dict_suv.csv
#   config_filename = "config.yaml",        # Name of user's config file
#   log_directory = user_log_dir,
#   log_file_prefix = paste0(run_tag, "_preprocess_log_"),
#   scandate_column = "ScanDate", # Or get from config if preprocess_data does that
#   verbose = TRUE
# )

# Option 2: To explicitly use package default config/dict,
# preprocess_data needs to be designed to find them if files_path is NULL or a specific flag is set.
# Assuming preprocess_data handles this:
# If files_path is NULL, it uses system.file("files", config_filename, package="biodiscvr")
preprocessed_output <- preprocess_data(
  loaded_data = loaded_data_raw,
  files_path = user_supporting_files_dir, # Set to NULL to force use of package default config/dict
                                          # OR ensure preprocess_data uses system.file if files_path is NULL
  config_filename = "config.yaml",        # This name is used with files_path or system.file
  dict_suv_filename = "dict_suv.csv",
  log_directory = user_log_dir,
  log_file_prefix = paste0(run_tag, "_preprocess_log_"),
  # scandate_column = "ScanDate", # This should ideally come from the config file
  verbose = TRUE
)


# Review the validation report for potential issues
# print("Validation Report Summary:")
# print(lapply(preprocessed_output$validation_report, names))

# The preprocessed data is in preprocessed_output$data
# The loaded config is in preprocessed_output$config
preprocessed_data_list <- preprocessed_output$data
config_loaded <- preprocessed_output$config # Use this config for subsequent steps
```

*Expected Output:* A list named `preprocessed_output` containing `$data` (the preprocessed data list), `$config` (loaded configuration), `$validation_report`, and `$log_file`. The `preprocessed_data_list` variable will hold the data for the next steps.

## Step 3: Creating Demographics Tables

Generates a summary table for each dataset based on the *preprocessed* data.

```{r demographics, eval=FALSE}
# Create output directory for demographics if it doesn't exist
if (!dir.exists(demographics_dir)) dir.create(demographics_dir, recursive = TRUE)

demographics_tables <- list()

for (dset_name in names(preprocessed_data_list)) {
  message("Generating demographics for: ", dset_name)
  demographics_csv_path <- file.path(demographics_dir, paste0("demographics_", dset_name, ".csv"))

  demographics_tables[[dset_name]] <- create_demographics_table(
    processed_dataset_list = preprocessed_data_list[[dset_name]],
    dataset_name = dset_name,
    output_csv_path = demographics_csv_path,
    id_col = config_loaded$preprocessing$id_column %||% "RID" # Use loaded config
  )
  # print(demographics_tables[[dset_name]])
}
```

*Expected Output:* CSV files saved in `demographics_dir`.

## Step 4: Running Discovery Experiments

Uses `run_experiments` based on your `experiments.yaml` (either custom or package default).

```{r run_experiments, eval=FALSE}
# Determine which experiments file to use
experiments_file_to_use <- if (!is.null(user_experiments_file) && file.exists(user_experiments_file)) {
  user_experiments_file
} else {
  message("User experiments file not found or not specified, using package default.")
  pkg_experiments_file # Defined in setup chunk
}
if (!file.exists(experiments_file_to_use)) stop("Experiments file not found!")

# Run the sequence of experiments
experiment_results_info <- run_experiments(
  prepared_data_list = preprocessed_data_list,
  config = config_loaded, # Use the config from preprocess_data output
  experiments_config_path = experiments_file_to_use,
  output_csv_name = main_results_csv,
  output_dir = file.path(user_output_dir),
  save_plots = TRUE,
  datasets_to_run = names(preprocessed_data_list),
  experiment_master_tag = run_tag,
  base_ga_seed = 42
)

message("Experiment runs complete. Main results appended to: ", main_results_csv)
```

*Expected Output:* `main_results_csv` populated. Plots saved.

## Step 5: Evaluating Literature Biomarkers

Evaluates biomarkers from your `literature_biomarkers.yaml` (custom or package default).

```{r eval_literature, eval=FALSE}
# Determine which literature definitions file to use
literature_file_to_use <- if (!is.null(user_literature_defs_file) && file.exists(user_literature_defs_file)) {
  user_literature_defs_file
} else {
  message("User literature biomarker file not found or not specified, using package default.")
  pkg_literature_defs_file # Defined in setup chunk
}
if (!file.exists(literature_file_to_use)) {
  warning("Literature biomarker file not found. Skipping evaluation.", immediate.=TRUE)
} else {
  literature_defs_yaml <- yaml::read_yaml(literature_file_to_use)
  literature_defs_list <- literature_defs_yaml$biomarkers

  literature_evaluation_df <- evaluate_literature_biomarkers(
    literature_defs = literature_defs_list,
    prepared_data_list = preprocessed_data_list,
    config = config_loaded,
    datasets_to_evaluate = names(preprocessed_data_list),
    groups_to_evaluate = c("CU", "CI"),
    calculate_ci = FALSE,
    output_evaluation_csv_path = literature_results_csv,
    verbose = TRUE
  )
  # print(head(literature_evaluation_df))
}
```

*Expected Output:* `literature_results_csv` populated.

## Reporting Output Files

After running the workflow, the key output files generated in your `user_output_dir` will be:

1.  **Main Discovery & Evaluation Results:**
    *   Path: `main_results_csv`
    *   Content: Results from `run_experiments`.

2.  **Literature Biomarker Evaluation Results:**
    *   Path: `literature_results_csv`
    *   Content: Performance metrics for standard biomarkers.

3.  **Demographics Tables:**
    *   Path: Inside the `demographics_dir`
    *   Files: One CSV per dataset.

4.  **Detailed Preprocessing Log File (Optional):**
    *   Path: Inside `user_log_dir`.

5.  **GA Convergence Plots (Optional):**
    *   Path: Inside the plot directory (e.g., `user_output_dir/plots/`).

Consult individual function help pages (`?function_name`) for more details.
