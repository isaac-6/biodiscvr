---
title: "Biodiscvr Workflow with User Data"
date: "`r Sys.Date()`"
toc-title: "Overview"
output: 
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Biodiscvr Workflow with User Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
# Load necessary packages for code display and explanation
library(biodiscvr)
library(yaml)
```

## Introduction
**Quick Start Note**: For a rapid, executed demonstration using pre-bundled synthetic data, please see the Case Demo: 
`vignette("biodiscvr_synth", package = "biodiscvr")`

The current vignette demonstrates the typical end-to-end workflow for using the `biodiscvr` package to discover and evaluate biomarkers using **your own data**.  

We will cover the following steps:

1.  Setting up necessary file paths.
2.  Loading datasets using `load_datasets()`.
3.  Preprocessing data (including checks, preparation, and filtering) using `preprocess_data()`.
4.  Generating demographic summaries using `create_demographics_table()`.
5.  Running discovery experiments using `run_experiments()`.
6.  Evaluating predefined literature biomarkers using `evaluate_literature_biomarkers()`.
7.  Identifying key output files for reporting.

### **Adapting the Framework to Other Domains**
While this vignette uses Alzheimer's Disease (AD) examples (e.g., amyloid status and neuroimaging regions), `biodiscvr` is designed to be a general-purpose biomarker discovery engine. To adapt this to a different field (such as Oncology or astronomy):

1.  **Define your Features:** Instead of brain regions, provide your domain-specific numeric features (e.g., gene expression levels, spectral intensities, imaging-derived metrics).
2.  **Define your Outcome:** In `config.yaml`, update the `group` definition to match your target outcome (e.g., positives vs. negatives, responders vs. non responders, high risk vs. low risk).
3.  **Map your Data:** Use `dict_suv.csv` (or an equivalent file) to map raw feature names to a standardised naming convention. This is especially useful when working with multiple datasets that use different labels: the framework uses this dictionary to harmonise names internally without modifying your original files. 

The underlying optimisation logic—finding optimal ratios that are robust across multiple independent datasets—remains the same regardless of the context.
If your application does not include longitudinal data, you may want to adapt the fitness function (e.g., using AUC, F1 score, or MAE instead of longitudinal consistency metrics).
For technical instructions on how to modify the source code or fitness metrics for non-AD research, please refer to the "Adapting to Other Research Domains" section in the [README](https://github.com/isaac-6/biodiscvr/tree/paper?tab=readme-ov-file#adapting-to-other-research-domains)


## Setup: User Files and Paths

BioDisCVR uses a decentralized loading system. Each cohort should be stored in its own folder within a main root directory. A typical setup looks like this:

```text
my_data_root/
├── Cohort_A/
│   ├── data.csv            # Metadata
│   └── data_suv_bi.csv     # Numeric features
└── Cohort_B/
    ├── data.csv
    └── data_suv_bi.csv
```

Before running the workflow, ensure you have the following files prepared and know their locations:

1.  **Root Data Directory:** The main folder containing subdirectories for each dataset (e.g., "ADNI", "OASIS"). Each subdirectory should contain at least `data.csv` and `data_suv_bi.csv`.
2.  **Configuration File (`config.yaml`):** Contains parameters for preprocessing (including `date_column` for time calculation, inclusion criteria), model equations, power calculations, and GA settings.
3.  **SUV Dictionary (`dict_suv.csv`):** Defines SUV region names for matching/renaming.
4.  **Experiments File (`experiments.yaml`):** Defines the sequence of discovery runs.
5.  **Literature Biomarkers File (`literature_biomarkers.yaml`):** Defines standard biomarkers for comparison.

You will also need to decide on output locations:

6.  **Output Directory:** A folder where results (CSV files, plots) will be saved.
7.  **Log Directory (Optional):** A folder to store detailed log files from `preprocess_data`.

**Package-Provided Files:**

This workflow can utilise default configuration files included with the `biodiscvr` package (located in its `inst/files` directory):
*   `config.yaml`: Default configuration.
*   `dict_suv.csv`: Default SUV dictionary.
*   `experiments.yaml`: Default set of discovery experiments.
*   `literature_biomarkers.yaml`: (To be created) Default set of literature biomarkers.

You can inspect these default files using `system.file("files", "<filename>", package = "biodiscvr")`. If you need to customise them, copy them from the package installation directory to your own project space and provide the path to your custom file instead of using the package default in the function calls below.

**Define Your Paths (Replace Placeholders):**

<details>
<summary><strong>Click to expand: Comprehensive Path Setup Code</strong></summary>
```r
# --- USER: Define paths to YOUR files and directories ---

# 1. Path to the main folder containing dataset subfolders
user_data_root_dir <- "path/to/your/data_root_folder" # <<< REPLACE

# 2. Path to the directory containing your CUSTOM config.yaml and dict_suv.csv
#    IF YOU ARE NOT USING THE PACKAGE DEFAULTS.
#    If using package defaults for config/dict, this can be NULL or point to where
#    check_and_prepare_data (now part of preprocess_data) expects them.
#    For preprocess_data, it will use system.file if files_path is not given for config/dict.
user_supporting_files_dir <- "path/to/your/supporting_files" # <<< REPLACE IF USING CUSTOM CONFIG/DICT
                                                          # OR set to NULL to use package defaults

# 3. Path to your CUSTOM experiments definition file (if not using package default)
user_experiments_file <- "path/to/your/experiments.yaml" # <<< REPLACE IF CUSTOM
#    To use package default: system.file("files", "experiments.yaml", package = "biodiscvr")

# 4. Path to your CUSTOM literature biomarker definition file (if not using package default)
user_literature_defs_file <- "path/to/your/literature_biomarkers.yaml" # <<< REPLACE IF CUSTOM
#    To use package default: system.file("files", "literature_biomarkers.yaml", package = "biodiscvr")


# 5. Path to the desired main output directory (will be created if needed)
user_output_dir <- "path/to/your/output_folder" # <<< REPLACE
if (!dir.exists(user_output_dir)) dir.create(user_output_dir, recursive = TRUE)

# 6. Path to the desired directory for detailed logs (optional, set to NULL to disable)
user_log_dir <- file.path(user_output_dir, "logs")

# 7. Define desired output CSV filenames
main_results_csv <- "discovery_evaluation_results.csv"
literature_results_csv <- file.path(user_output_dir, "literature_evaluation_results.csv")
demographics_dir <- file.path(user_output_dir, "demographics")

# 8. Define datasets to process (must match folder names in user_data_root_dir)
datasets_to_process <- c("ADNI", "MAYO") # <<< REPLACE

# 9. Define an overall tag for this experimental run
run_tag <- format(Sys.time(), "%Y%m%d_%H%M")

# --- Get Paths to Package-Internal Files (if user_supporting_files_dir is NULL for config/dict) ---
# preprocess_data will handle finding these if files_path is not provided to it.
# We still need the path for experiments_file if using package default.
pkg_experiments_file <- system.file("files", "experiments.yaml", package = "biodiscvr", mustWork = TRUE)
pkg_literature_defs_file <- system.file("files", "literature_biomarkers.yaml", package = "biodiscvr", mustWork = TRUE) # Assuming it exists

# --- Load Config Once (needed by multiple steps) ---
# If using a custom config file:
# custom_config_path <- file.path(user_supporting_files_dir, "config.yaml")
# if (!file.exists(custom_config_path)) stop("Custom config file not found!")
# config <- yaml::read_yaml(custom_config_path)
# If using package default config (preprocess_data will load it):
# For other functions, we'll get it from preprocess_data output.

```
</details>

## Loading Datasets

This step scans your `user_data_root_dir` for subdirectories matching `datasets_to_process` and loads the `data.csv` and `data_suv_bi.csv` files from each.

```{r load_data, eval=FALSE}
# Load the datasets specified
loaded_data_raw <- load_datasets(root_path = user_data_root_dir)
# print(str(loaded_data_raw, max.level = 2))
```

*Expected Output:* A list named `loaded_data_raw`.

## Preprocessing Data (Checks, Preparation, Filtering)

This single function handles:
*   Validating loaded data against configuration and dictionary.
*   Calculating the `time` variable from a date column (e.g., `ScanDate`).
*   Performing SUV column matching and renaming/subsetting.
*   Applying filtering based on minimum entries and group-specific inclusion criteria.
*   Sets up logging.

If `user_supporting_files_dir` is provided and contains `config.yaml` and `dict_suv.csv`, those will be used. Otherwise, `preprocess_data` will attempt to use the default versions from `inst/files/` within the package.

```{r preprocess_data_merged, eval=FALSE}
# If files_path is NULL, it uses system.file("files", config_filename, package="biodiscvr")
preprocessed_output <- preprocess_data(
  loaded_data = loaded_data_raw,
  files_path = user_supporting_files_dir, # Set to NULL to force use of package default config/dict
                                          # OR ensure preprocess_data uses system.file if files_path is NULL
  config_filename = "config.yaml",        # This name is used with files_path or system.file
  dict_suv_filename = "dict_suv.csv",
  log_directory = user_log_dir,
  log_file_prefix = paste0(run_tag, "_preprocess_log_"),
  # scandate_column = "ScanDate", # This should ideally come from the config file
  verbose = TRUE
)

preprocessed_data_list <- preprocessed_output$data
config_loaded <- preprocessed_output$config # Use this config for subsequent steps
```

*Expected Output:* A list named `preprocessed_output` containing `$data` (the preprocessed data list), `$config` (loaded configuration), `$validation_report`, and `$log_file`. The `preprocessed_data_list` variable will hold the data for the next steps.

## Creating Demographics Tables

Generates a summary table for each dataset metadata based on the *preprocessed* files.

```{r demographics, eval=FALSE}
# Create output directory for demographics if it doesn't exist
if (!dir.exists(demographics_dir)) dir.create(demographics_dir, recursive = TRUE)

demographics_tables <- list()

for (dset_name in names(preprocessed_data_list)) {
  message("Generating demographics for: ", dset_name)
  demographics_csv_path <- file.path(demographics_dir, paste0("demographics_", dset_name, ".csv"))

  demographics_tables[[dset_name]] <- create_demographics_table(
    processed_dataset_list = preprocessed_data_list[[dset_name]],
    dataset_name = dset_name,
    output_csv_path = demographics_csv_path,
    id_col = config_loaded$preprocessing$id_column %||% "RID" # Use loaded config
  )
  # print(demographics_tables[[dset_name]])
}
```

*Expected Output:* CSV files saved in `demographics_dir`.

## Running Discovery Experiments
This phase involves the automated search for an optimal biomarker ratio. The package uses a Genetic Algorithm (GA) to navigate the combinatorial space of feature groupings.  
The `run_experiments()` function executes a batch of discovery runs ( from `experiments.yaml`) across your selected cohorts. It prioritises biomarkers that achieve a high **consensus fitness**, meaning they perform well and consistently across independent cohorts.

```{r run_experiments, eval=FALSE}
# Determine which experiments file to use
experiments_file_to_use <- if (!is.null(user_experiments_file) && file.exists(user_experiments_file)) {
  user_experiments_file
} else {
  message("User experiments file not found or not specified, using package default.")
  pkg_experiments_file # Defined in setup chunk
}
if (!file.exists(experiments_file_to_use)) stop("Experiments file not found!")

# Run the sequence of experiments
experiment_results_info <- run_experiments(
  prepared_data_list = preprocessed_data_list,
  config = config_loaded, # Use the config from preprocess_data output
  experiments_config_path = experiments_file_to_use,
  output_csv_name = main_results_csv,
  output_dir = file.path(user_output_dir),
  save_plots = TRUE,
  datasets_to_run = names(preprocessed_data_list),
  experiment_master_tag = run_tag,
  base_ga_seed = 42
)

message("Experiment runs complete. Main results appended to: ", main_results_csv)
```

*Expected Output:* `main_results_csv` populated. Plots saved.

## Evaluating Literature Biomarkers

Evaluates biomarkers from your `literature_biomarkers.yaml` (custom or package default).

```{r eval_literature, eval=FALSE}
# Determine which literature definitions file to use
literature_file_to_use <- if (!is.null(user_literature_defs_file) && file.exists(user_literature_defs_file)) {
  user_literature_defs_file
} else {
  message("User literature biomarker file not found or not specified, using package default.")
  pkg_literature_defs_file # Defined in setup chunk
}
if (!file.exists(literature_file_to_use)) {
  warning("Literature biomarker file not found. Skipping evaluation.", immediate.=TRUE)
} else {
  literature_defs_yaml <- yaml::read_yaml(literature_file_to_use)
  literature_defs_list <- literature_defs_yaml$biomarkers

  literature_evaluation_df <- evaluate_literature_biomarkers(
    literature_defs = literature_defs_list,
    prepared_data_list = preprocessed_data_list,
    config = config_loaded,
    datasets_to_evaluate = names(preprocessed_data_list),
    groups_to_evaluate = c("CU", "CI"),
    calculate_ci = FALSE,
    output_evaluation_csv_path = literature_results_csv,
    verbose = TRUE
  )
  # print(head(literature_evaluation_df))
}
```

*Expected Output:* `literature_results_csv` populated.

## Feature Ablation
In case you are wondering "is there a subset of CVR that performs better on dataset X?": the `run_ablation()` function performs a "feature removal" analysis, identifying which regions are essential and which can be removed without sacrificing statistical power.

```{r ablation}
ablation_results <- run_ablation(
  discovered_biomarkers_csv_path = file.path(user_output_dir, main_results_csv),
  prepared_data_list = data_list,
  config = config,
  output_ablation_results_csv_path = file.path(user_output_dir, "refined_biomarker.csv")
)
```
*Expected Output:* `refined_biomarker.csv` populated with the biomarkers' sample size estimates before and after feature ablation, and with detailed features before and after.

## Reporting Output Files

After running the workflow, the key output files generated in your `user_output_dir` will be:

0.  **Main Discovery & Evaluation Results:**
    *   Path: `main_results_csv`
    *   Content: Results from `run_experiments`.
    
1.  **Ablation Results:**
    *   Path: `refined_biomarker.csv` inside `user_output_dir`
    *   Content: Results from `run_ablation`.

2.  **Literature Biomarker Evaluation Results:**
    *   Path: `literature_results_csv`
    *   Content: Performance metrics for standard biomarkers.

3.  **Demographics Tables:**
    *   Path: Inside the `demographics_dir`
    *   Files: One CSV per dataset.

4.  **Detailed Preprocessing Log File (Optional):**
    *   Path: Inside `user_log_dir`.

5.  **GA Convergence Plots (Optional):**
    *   Path: Inside the plot directory (e.g., `user_output_dir/plots/`).

Consult individual function help pages (`?function_name`) for more details.
